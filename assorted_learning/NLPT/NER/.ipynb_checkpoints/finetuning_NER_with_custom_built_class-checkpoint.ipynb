{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "834dceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a named entity recognition model from scratch.\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from seqeval.metrics import f1_score\n",
    "from torch.nn.functional import cross_entropy\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8dc0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the serialized data\n",
    "with open('../../data/panx_ch_short_data.pkl', 'rb') as fh:\n",
    "    panx_ch = pickle.load(fh)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "281379bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner tags\n",
    "tags = panx_ch['de']['train'].features['ner_tags'].feature  # a classLabel object\n",
    "idx2tags = {idx:tag for idx, tag in enumerate(tags.names)}\n",
    "tags2idx = {tag:idx for idx, tag in enumerate(tags.names)}\n",
    "\n",
    "model_checkpoint = 'xlm-roberta-base'\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "xlmr_config = AutoConfig.from_pretrained(model_checkpoint,\n",
    "                                         id2label=idx2tags,\n",
    "                                         label2id=tags2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f09d93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['XLMRoBERTaForTokenClassificationCustom'], 'finetuning_task': None, 'id2label': {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}, 'label2id': {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 0, 'pad_token_id': 1, 'eos_token_id': 2, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'xlm-roberta-base', 'transformers_version': '4.28.1', 'model_type': 'xlm-roberta', 'output_past': True, 'vocab_size': 250002, 'hidden_size': 768, 'num_hidden_layers': 12, 'num_attention_heads': 12, 'hidden_act': 'gelu', 'intermediate_size': 3072, 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 514, 'type_vocab_size': 1, 'initializer_range': 0.02, 'layer_norm_eps': 1e-05, 'position_embedding_type': 'absolute', 'use_cache': True, 'classifier_dropout': None}\n"
     ]
    }
   ],
   "source": [
    "print(xlmr_config.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f4f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(lazy_batch):\n",
    "    tokenized_inputs = xlmr_tokenizer(lazy_batch['tokens'], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for ner_tag_idx, ner_tag in enumerate(lazy_batch['ner_tags']):\n",
    "        label_ids = []\n",
    "        previous_token_id = None\n",
    "        for token_id in tokenized_inputs.word_ids(ner_tag_idx):\n",
    "            # we don't want to calculate loss for the starting and ending token which has been labelled as None.\n",
    "            # Also, if a word is split into multiple tokens, we calculate loss only for the first token.\n",
    "            # if a token is labelled -100 its loss shouldn't be calculated.\n",
    "            if token_id is None or token_id == previous_token_id:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(ner_tag[token_id])\n",
    "            previous_token_id = token_id\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6bf99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_panx_dataset(panx_ch):\n",
    "    return panx_ch.map(tokenize_and_align_labels,\n",
    "                       batched=True,\n",
    "                       batch_size=1000,\n",
    "                       remove_columns=['tokens', 'ner_tags', 'langs'])\n",
    "\n",
    "\n",
    "panx_de_encoded = encode_panx_dataset(panx_ch['de'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed16c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRoBERTaForTokenClassificationCustom(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # RobertaModel is a bare-bones model that output 768 dimensional features\n",
    "        self.roberta = RobertaModel(config=config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                labels=None,\n",
    "                **kwargs):\n",
    "        outputs = self.roberta(input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               **kwargs)\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return TokenClassifierOutput(loss=loss,\n",
    "                                     logits=logits,\n",
    "                                     hidden_states=outputs.hidden_states,\n",
    "                                     attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d80af20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return XLMRoBERTaForTokenClassificationCustom.from_pretrained(model_checkpoint,\n",
    "                                                       config=xlmr_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "788eb817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_predictions(label_ids, predictions):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    labels_lst, predictions_lst = [], []\n",
    "    batch_size, seq_len = preds.shape\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(idx2tags[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(idx2tags[preds[batch_idx][seq_idx]])\n",
    "        labels_lst.append(example_labels)\n",
    "        predictions_lst.append(example_preds)\n",
    "    return labels_lst, predictions_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed91c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_merics(eval_prediction):\n",
    "    y_true, y_pred = align_predictions(eval_prediction.label_ids,\n",
    "                                       eval_prediction.predictions)\n",
    "    return {'f1':f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00d98f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karvsmech/miniconda3/envs/ptorch/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 00:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.600400</td>\n",
       "      <td>0.240534</td>\n",
       "      <td>0.739373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.221800</td>\n",
       "      <td>0.190291</td>\n",
       "      <td>0.781415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>0.190468</td>\n",
       "      <td>0.791286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=0.3216189843006235, metrics={'train_runtime': 48.4183, 'train_samples_per_second': 123.92, 'train_steps_per_second': 3.903, 'total_flos': 142453999337760.0, 'train_loss': 0.3216189843006235, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model training arguments\n",
    "num_epochs=3\n",
    "batch_size=32\n",
    "logging_steps = len(panx_de_encoded['train']) // batch_size\n",
    "model_name = f'{model_checkpoint}_finetuned_panx_de'\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  log_level='error',\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  learning_rate=5e-5,\n",
    "                                  weight_decay=0.01,\n",
    "                                  num_train_epochs=num_epochs,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  save_steps=1e6, # avoiding saving checkpoints\n",
    "                                  seed=42,\n",
    "                                  fp16=False,\n",
    "                                  disable_tqdm=False,\n",
    "                                  push_to_hub=False\n",
    "                                  )\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)\n",
    "trainer = Trainer(model_init=model_init,\n",
    "                  args=training_args,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=panx_de_encoded['train'],\n",
    "                  eval_dataset=panx_de_encoded['validation'],\n",
    "                  compute_metrics=compute_merics,\n",
    "                  tokenizer=xlmr_tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7833c049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../tokenizer_checkpoints/ner_tokenizer_for_token_classification/tokenizer_config.json',\n",
       " '../../tokenizer_checkpoints/ner_tokenizer_for_token_classification/special_tokens_map.json',\n",
       " '../../tokenizer_checkpoints/ner_tokenizer_for_token_classification/sentencepiece.bpe.model',\n",
       " '../../tokenizer_checkpoints/ner_tokenizer_for_token_classification/added_tokens.json',\n",
       " '../../tokenizer_checkpoints/ner_tokenizer_for_token_classification/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the model and the tokenizer.\n",
    "model = trainer.model\n",
    "tokenizer = trainer.tokenizer\n",
    "\n",
    "model.save_pretrained('../../model_checkpoints/ner_model_for_token_classification')\n",
    "tokenizer.save_pretrained('../../tokenizer_checkpoints/ner_tokenizer_for_token_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c52f16f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jeff</td>\n",
       "      <td>▁De</td>\n",
       "      <td>an</td>\n",
       "      <td>▁ist</td>\n",
       "      <td>▁ein</td>\n",
       "      <td>▁Informati</td>\n",
       "      <td>ker</td>\n",
       "      <td>▁bei</td>\n",
       "      <td>▁Google</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁Kalo</td>\n",
       "      <td>for</td>\n",
       "      <td>ni</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NER</th>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3     4     5           6    7     8        9   \\\n",
       "tokens  <s>  ▁Jeff    ▁De     an  ▁ist  ▁ein  ▁Informati  ker  ▁bei  ▁Google   \n",
       "NER       O  B-PER  I-PER  I-PER     O     O           O    O     O    B-ORG   \n",
       "\n",
       "         10     11     12     13     14    15  \n",
       "tokens  ▁in  ▁Kalo    for     ni     en  </s>  \n",
       "NER       O  B-LOC  I-LOC  I-LOC  I-LOC     O  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tag_text(text, tokenizer):\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    input_ids = tokenizer(text, return_tensors='pt').input_ids.to(device)\n",
    "    class_label = torch.argmax(trainer.model(input_ids).logits, axis=-1)\n",
    "    class_label = class_label[0].cpu().numpy()\n",
    "    predictions = [idx2tags[idx] for idx in class_label]\n",
    "    return pd.DataFrame([tokens, predictions], index=['tokens', 'NER'])\n",
    "\n",
    "text = 'Jeff Dean ist ein Informatiker bei Google in Kalofornien'\n",
    "tag_text(text, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056dc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd121b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f6788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84979ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b31dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f52a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef091ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6b3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfc501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327eb05c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptorch",
   "language": "python",
   "name": "ptorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
